---
title: 算法导论(3) 哈希
published: 2025-09-17
description: ''
image: ''
tags: []
category: ''
draft: false 
lang: ''
---

## 字典(Dictionary)
一种抽象数据类型. 其维护某个集合, 集合中的每个元素`item`都含有键`key`.
拥有如下操作:
1. 插入: `insert(item)`
2. 删除: `delete(item)`
3. 查找: `search(key)`

python的内置类**字典**实现了上述功能, 每个元素都为一个键-值对.

我们知道, 平衡BST能在$O(log n)$内解决如上问题.

**能不能更快?**

## 直接寻址表(Direct Access Table)
最简单的想法就是建立一个数组, 由于数组的(指定索引)随机访问的效率是$O(1)$, 可以再设置一个状态保存空元素. 于是:
1. `insert(key, value): arr[key] = value`
2. `delete(key, value): arr[key] = EMPTY`
3. `search(key): return arr[key]`

我们就完成了上述目标. 然而聪明的同学可能已经发现, 我们这个实现有很多问题:
1. 最主要的问题是`key`只能是非负整数, 而我们往往需要以字符串或者其他类型作为`key`.
2. 假如key的分布比较稀疏(e.g. `key[0] = 1, key[1] = 100000000000`), 那么即使我们只需要存两个元素, 也得开`100000000000`以上的空间!

### 问题1的解决: prehash
将任意类型的键，确定性地转换成一个整数. 
我们暂时不讲这个转换方法, 后面会讲到. 目前只需要知道是可以做到把任意类型的键映射到一个整数的就行.
python内置了这个方法, 即`hash()`函数. 对于自定义对象可以使用`__hash__`方法.

### 问题2的解决: hashing
我们现在有了一个(可能非常大的)整数键. 我们需要将这个巨大的整数域(称为全域U), 映射到一个大小合理的, 可以作为数组索引的小范围 $[0, 1, ..., m-1]$ 中. $m$就是我们哈希表(数组)的大小.
这个映射叫做哈希函数(hash function). 这里同样不讲哈希函数的具体实现.

通过预哈希和哈希函数可以将n个不同元素映射到m个不同的桶(或槽位, slot)中. 在理想情况下m = n, 并且每个桶恰好储存一个元素. 这样既不会浪费空间, 也能精确查找.

然而我们发现, 这是难以做到的.

## 哈希碰撞/哈希冲突
我们将一个巨大的整数域映射到了一个小范围里, 由于所有可能的键的数量远大于m, 根据抽屉原理, 必然会有多个不同的键$k_1, k_2, ... k_s$被映射到同一个桶中. 假如说我们的元素恰好被prehash到了$k_1, k_2, ... k_s$的某些键当中, 就必然出现同一个桶里储存了不同的元素. 这就叫做**哈希碰撞**(或哈希冲突, Collisions).

如何处理冲突，是整个哈希技术的核心和灵魂. 我们课程里会介绍两种主流的冲突解决方式. 这里介绍**拉链法**, 后续课程中会介绍**开放寻址法**.

### 拉链法(Chaining)
让哈希表的每个桶不止储存一个元素, 而是储存一个链表:
1. `insert(key, value): arr[key].append(value)`
2. `delete(key, value): arr[key].remove(value)`
3. `search(key): return arr[key].find(value)`

好结局: 所有数据均匀分配到各个桶中, 我们只需要花费常数复杂度的代价即可解决哈希冲突.

坏结局: 我们的运气极差, 所有 $n$ 个键都被哈希到了同一个桶中. 这时, 哈希表退化成了一个长链表, 所有操作的时间复杂度都变成了$O(n)$, 我们的 $O(1)$ 梦想破灭了. <del>(建议买彩票)</del>

为了(在大多数情况下)避免坏结局, 我们必须让键均匀地分配到各个桶中. 让每个链表都尽可能短.

### 简单均匀哈希假设(Simple Uniform Hashing Assumption)
一个理想化的哈希函数, 所有数据分配到各个桶中的概率均等(为1/m)并且互相独立. 

这时, 每个桶中的链表长度期望值为n / m, 称为**负载因子**(Load Factor), 记作
$$
\alpha = \frac{n}{m}
$$
.

简单均匀哈希假设下, 所有操作的期望平均时间复杂度为$\Theta (1 + \alpha)$. 那么我们如果能够把负载因子保持在一个常数, 这个复杂度就是$\Theta(1)$.

由于$\alpha = \frac{n}{m}$, 也就是说, 我们需要让$n$与$m$成正比.

这是一个理想化的假设, 我们的目标是设计出尽可能接近这个假设的哈希函数.

### 除法哈希
除法哈希, 或者叫模哈希. $hashing(k) = k \% m$.
优点在于简单快速. 关键在于m的选择, 为了避开数据可能的分布规律, m不应被选择为2的整数次幂或10的整数次幂.

一般来说, 我们选择m为一个远离2的幂次的质数.

### 乘法哈希
核心思想:
1. 将键k乘以某个数a. 乘法这个操作, 特别是当数字很大并发生溢出时, 具有非常好的“雪崩效应”: 键k的微小变化, 都会导致乘积结果的巨大变化, 所有位数都会被充分地打乱.
2. 从这个被打乱的结果中, 我们不像除法那样取低位, 而是**提取中间或高位的某一部分**作为最终的哈希值. 因为乘积的中间部分比特, 是受输入键k的**所有位**共同影响的结果, 因此它包含了最"混乱", 分布最均匀的信息.

讲义上其实提到了两种形式的乘法哈希.

:::tip[理论数学形式]
1. 挑选一个数$0<A<1$, 比如可以取黄金分割比0.618. 将键$k$乘以$A$.
2. 取$kA$的小数部分, 记为$c$, 这个小数部分对k的变化非常敏感.
3. 这里知道$0<c<1$, 我们将其乘$m$, $0<cm<m$
4. 对$cm$向下取整, 即得到了我们要的$hash(k)$

想象一个周长为1的圆. 乘以$A$, 就像在圆盘上旋转一个固定距离$A$. $kA$就是旋转$k$次. $c$就是旋转$k$次后的最终位置, 这个位置看起来是相当随机的. 然后我们把圆盘分成$m$个扇区，看它落在了哪个扇区, 这就是哈希值.

这个方法优美而且易于理解, 然而其涉及浮点数的精确运算, 在计算机里不容易实现. 于是我们有:
:::

:::tip[计算机实现形式]
假设`k`不会超过`w`位. `w`一般取32位(`k`为`uint32_t`)或64位(`k`为`uint64_t`)
`m`为哈希表的大小. 为了方便, 可以设置为2的r次幂, 其中r为整数, 即`m = 1 << r`
1. 挑选一个`w`位的数`a`, 通常是一个较大的, 远离2的整数次幂的奇数. 将键`k`乘以`a`. 结果会是一个`2w`位的整数.
2. 由于我们的`w`取的刚好是`uint32_t`类型或者`uint64_t`类型的上限, 这个乘法会自然溢出, 只剩下后`w`位, 记作`c`.
3. 我们要取中间位, 由于高位已经被我们舍弃掉了, 所以只需要舍弃一些低位即可. 具体来说我们要保留高`r`位, 只需将`c`右移到只剩`r`位即可. 即`hash(k) = c >> (w - r)`
:::

乘法哈希的优点:
1. 速度快: 只利用CPU中最为高效的指令, 左右移和乘法, 比取模运算优秀.
2. 对m的选择友好: m可以取2的整数次幂, 在计算机中非常方便. 反观除法哈希需要寻找一个大质数, 这通常是非常慢的.
3. 打乱效果好, 几乎不会受数据内在一般规律的影响. 有效避免冲突.

### 全域哈希
以上的哈希函数都是确定性的. 如果攻击者知道了我们的哈希函数, 他/她可以精心构造出一组数据, 使它们全部发生哈希碰撞, 从而使我们的系统性能降低至$O(n)$. (也就是算法竞赛选手熟知的**卡哈希**)

全域哈希的思想: 我们不再使用一个固定的哈希函数, 而是使用一整个哈希函数族$H$. 程序开始时, 我们在函数族$H$中随机选择一个哈希函数$h$作为本次运行时的哈希函数. 这意味着即使攻击者知道了整个哈希函数族$H$, 他/她也无法预测我们这次会用哪个函数, 从而无法构造出对于特定函数的最坏情况. 因此对于任何输入我们都能有良好的平均性能.

例: $hash_{a,b}(k) = [(ak+b) \%p]\%m$. 其中$a, b$是随机选择的, $p$是一个大质数, 如$998244353$.